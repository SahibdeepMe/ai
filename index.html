<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Jarvis AI Assistant</title>
<style>
    * { box-sizing: border-box; }

    body {
        margin: 0;
        background: radial-gradient(circle at center, #001a1a 0%, #000000 100%);
        color: #00fff6;
        font-family: 'Orbitron', sans-serif;
        display: flex;
        align-items: center;
        justify-content: center;
        height: 100vh;
        flex-direction: column;
        overflow: hidden;
    }

    #avatar {
        width: 180px;
        height: 180px;
        background: radial-gradient(circle, #00fff6, #004d4d);
        border-radius: 50%;
        display: flex;
        align-items: center;
        justify-content: center;
        box-shadow: 0 0 40px #00fff6;
        animation: pulse 2s infinite ease-in-out;
        font-size: 3rem;
        user-select: none;
    }

    @keyframes pulse {
        0%, 100% { box-shadow: 0 0 20px #00fff6; }
        50% { box-shadow: 0 0 50px #00fff6; transform: scale(1.05); }
    }

    #status {
        margin-top: 25px;
        font-size: 1.2rem;
        text-shadow: 0 0 10px #00fff6;
    }

    #startBtn, #proceedBtn {
        margin-top: 30px;
        padding: 12px 28px;
        background: #00fff6;
        color: #001a1a;
        border: none;
        border-radius: 8px;
        cursor: pointer;
        font-size: 1rem;
        font-weight: bold;
        box-shadow: 0 0 15px #00fff6;
        transition: 0.3s;
    }

    #startBtn:hover, #proceedBtn:hover {
        background: #00cccc;
        box-shadow: 0 0 30px #00fff6;
    }

    #transcript {
        margin-top: 30px;
        font-size: 1rem;
        color: #ffffff;
        text-align: center;
        max-width: 80%;
        line-height: 1.5;
    }

    #topControls {
        position: absolute;
        top: 15px;
        width: 100%;
        display: flex;
        justify-content: space-between;
        padding: 0 20px;
    }

    .control-btn {
        background: transparent;
        border: 2px solid #00fff6;
        color: #00fff6;
        padding: 8px 14px;
        border-radius: 6px;
        cursor: pointer;
        font-size: 0.9rem;
        text-shadow: 0 0 10px #00fff6;
        transition: 0.3s;
    }

    .control-btn:hover {
        background: #00fff6;
        color: #001a1a;
        box-shadow: 0 0 20px #00fff6;
    }

</style>
</head>
<body>
    <div id="topControls">
        <button id="chatToggle" class="control-btn">üí¨ Chat Mode: Off</button>
        <button id="muteToggle" class="control-btn">üîá Mute</button>
    </div>

    <div id="avatar">üéôÔ∏è</div>
    <div id="status">Press "Start Jarvis" to begin.</div>
    <button id="startBtn">Start Jarvis</button>
    <div id="transcript"></div>
    <button id="proceedBtn">Proceed</button>

    <script>
        const DEEPSEEK_API_KEY = "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";
        const DEEPSEEK_API_URL = "https://openrouter.ai/api/v1/chat/completions";
        const MODEL = "deepseek/deepseek-chat";

        const statusElement = document.getElementById("status");
        const transcriptElement = document.getElementById("transcript");
        const startBtn = document.getElementById("startBtn");
        const proceedBtn = document.getElementById("proceedBtn");
        const muteToggle = document.getElementById("muteToggle");
        const chatToggle = document.getElementById("chatToggle");

        let recognition;
        let lastQuestion = "";
        let isMuted = false;
        let chatMode = false;

        function speak(text, callback) {
            if (isMuted || chatMode) return callback && callback();
            const cleanedText = text.replace(
                /([\u2700-\u27BF]|[\uE000-\uF8FF]|\uD83C[\uDC00-\uDFFF]|\uD83D[\uDC00-\uDFFF]|\uD83E[\uDD00-\uDDFF])/g,
                ''
            );
            const utterance = new SpeechSynthesisUtterance(cleanedText);
            utterance.lang = 'en-US';
            utterance.onend = () => callback && callback();
            window.speechSynthesis.speak(utterance);
        }

        async function queryDeepSeek(prompt) {
            try {
                const res = await fetch(DEEPSEEK_API_URL, {
                    method: "POST",
                    headers: {
                        "Authorization": `Bearer ${DEEPSEEK_API_KEY}`,
                        "Content-Type": "application/json"
                    },
                    body: JSON.stringify({
                        model: MODEL,
                        messages: [{ role: "user", content: prompt }]
                    })
                });
                const data = await res.json();
                return data.choices?.[0]?.message?.content || "I couldn't find an answer.";
            } catch (e) {
                console.error("DeepSeek API error:", e);
                return "Error fetching data.";
            }
        }

        function startRecognition() {
            const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
            if (!SpeechRecognition) {
                statusElement.innerText = "Speech Recognition not supported.";
                return;
            }

            recognition = new SpeechRecognition();
            recognition.lang = 'en-US';
            recognition.continuous = true;
            recognition.interimResults = false;

            recognition.onstart = () => {
                if (!isMuted && !chatMode) statusElement.innerText = "Listening...";
            };

            recognition.onresult = (event) => {
                if (isMuted || chatMode) return;
                const newSpeech = event.results[event.results.length - 1][0].transcript.trim();
                transcriptElement.innerText = "You said: " + newSpeech;

                if (newSpeech.toLowerCase().includes("hey jarvis")) {
                    if (window.speechSynthesis.speaking) window.speechSynthesis.cancel();
                    speak("Yes sir, I am ready. Ask your question.");
                    return;
                }

                if (window.speechSynthesis.speaking) window.speechSynthesis.cancel();
                lastQuestion = newSpeech;
                proceedBtn.style.display = "inline-block";
            };

            recognition.onerror = (event) => {
                console.error("Recognition error:", event.error);
                statusElement.innerText = "Error: " + event.error;
            };

            recognition.onend = () => {
                if (!isMuted && !chatMode) recognition.start();
            };

            recognition.start();
        }

        proceedBtn.addEventListener("click", async () => {
            proceedBtn.style.display = "none";
            statusElement.innerText = "Searching...";
            if (!chatMode) speak("Searching from the web...", async () => {
                const answer = await queryDeepSeek(lastQuestion);
                statusElement.innerText = "Loading your answer...";
                speak("Loading your answer...", () => {
                    speak(answer, () => {
                        transcriptElement.innerHTML = "<b>Jarvis:</b> " + answer;
                        lastQuestion = "";
                    });
                });
            });
            else {
                const answer = await queryDeepSeek(lastQuestion);
                transcriptElement.innerHTML = "<b>Jarvis:</b> " + answer;
                statusElement.innerText = "Answer ready.";
            }
        });

        startBtn.addEventListener("click", async () => {
            try {
                statusElement.innerText = "Requesting microphone access...";
                await navigator.mediaDevices.getUserMedia({ audio: true });
                statusElement.innerText = "Jarvis is listening.";
                startBtn.style.display = "none";
                if (!chatMode && !isMuted) startRecognition();
            } catch (err) {
                console.error("Mic access denied:", err);
                statusElement.innerText = "Microphone access denied.";
            }
        });

        muteToggle.addEventListener("click", () => {
            isMuted = !isMuted;
            muteToggle.textContent = isMuted ? "üîà Unmute" : "üîá Mute";
            statusElement.innerText = isMuted ? "Jarvis muted and stopped listening." : "Jarvis active.";
            if (isMuted && recognition) recognition.stop();
            if (!isMuted && !chatMode) startRecognition();
        });

        chatToggle.addEventListener("click", () => {
            chatMode = !chatMode;
            chatToggle.textContent = chatMode ? "üí¨ Chat Mode: On" : "üí¨ Chat Mode: Off";
            statusElement.innerText = chatMode ? "Chat mode activated (no voice)." : "Voice mode activated.";
            if (chatMode && recognition) recognition.stop();
            if (!chatMode && !isMuted) startRecognition();
        });

        window.onload = () => {
            speak("Jarvis initialized and ready.");
        };
    </script>
</body>
</html>
